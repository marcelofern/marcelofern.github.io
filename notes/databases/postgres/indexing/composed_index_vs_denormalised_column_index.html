<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <style rel="stylesheet">
    body{
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      margin:40px auto;
      max-width:750px;
      font-size:18px;
      padding:0 10px;
      color: #333;
    }
    pre {
      background: #f4f4f4;
      border: 1px solid #ddd;
      border-left: 3px solid #000;
      color: #777;
      page-break-inside: avoid;
      font-family: monospace;
      font-size: 15px;
      max-width: 100%;
      overflow: auto;
      padding: 1em 1.5em;
      display: block;
      word-wrap: break-word;
    }
    a {
      color: #1976d2;
      text-decoration: none;
      border-bottom: 1px solid;
    }
    img {
      max-width: 100%;
    }
    h1 {
      text-align: left;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    li {
      margin-bottom: 10px;
    }
    :not(pre) > code {
      background-color: #f4f4f4;
      padding-right: 0.2em;
      padding-left: 0.2em;
      border-radius: 3px;
    }
  </style>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h1>Composed Index vs Denormalised Column Index</h1>
<pre><code>Created at: 2024-12-02
</code></pre>
<p>Sometimes it may sound like a good idea to denormalise a couple of columns so
that an index lookup can be performed faster.</p>
<p>But does it actually?</p>
<h2>Tables</h2>
<p>Imagine the following table:</p>
<pre><code class="language-sql">CREATE TABLE normalised (
    id SERIAL PRIMARY KEY,
    foo varchar,
    bar varchar
);
CREATE INDEX idx_foo_bar ON normalised (foo, bar);
</code></pre>
<p>And its denormalised version, where <code>foo</code> and <code>bar</code> are stored in foobar as
<code>foo:bar</code>.</p>
<pre><code class="language-sql">CREATE TABLE denormalised (
    id SERIAL PRIMARY KEY,
    foo varchar,
    bar varchar,
    foobar varchar
);
CREATE INDEX idx_foobar ON denormalised (foobar);
</code></pre>
<p>The normalised version is actually faster for queries like:</p>
<pre><code class="language-sql">SELECT foo, bar FROM normalised where foo='{foo}' and bar ='{bar}';
</code></pre>
<p>Instead of the denormalised version</p>
<pre><code class="language-sql">SELECT foobar FROM denormalised where foobar='{foo}:{bar}';
</code></pre>
<p>The normalised version has to read less buffers for performing the lookups and
is a tidy bit faster.</p>
<p>I built a benchmark script for testing this scenario against a table with 10
million rows:</p>
<pre><code class="language-py">import time
import re
import random
import psycopg2

# Change these before running locally.
DB_NAME = &quot;composed_idx&quot;
USER = &quot;marcelo.fernandes&quot;
PASSWORD = &quot;&quot;
HOST = &quot;localhost&quot;
PORT = 5415
NUM_OF_ROWS = 10_000_000
INSERT_ROWS_PER_BATCH = 1000_000
NUM_OF_QUERIES = 30_000

# Uncomment these on local dev for tests to run fast.
# NUM_OF_ROWS = 10_000
# INSERT_ROWS_PER_BATCH = 1000
# NUM_OF_QUERIES = 1


def get_cursor_and_connection():
    conn = psycopg2.connect(
        dbname=DB_NAME,
        user=USER,
        password=PASSWORD,
        host=HOST,
        port=PORT,
    )
    conn.autocommit = True
    return conn.cursor(), conn


def create_normalised_table(cursor):
    print(&quot;- Creating normalised table...&quot;)
    cursor.execute(&quot;&quot;&quot;
        -- Idempotency for convenience.
        DROP TABLE IF EXISTS normalised;
        CREATE TABLE normalised (
            id SERIAL PRIMARY KEY,
            foo varchar,
            bar varchar
        );
        -- Disable autovacuum to not interfere with results.
        ALTER TABLE normalised SET (autovacuum_enabled = false);

        CREATE INDEX idx_foo_bar ON normalised (foo, bar);
    &quot;&quot;&quot;)


def create_denormalised_table(cursor):
    print(&quot;- Creating denormalised table...&quot;)
    cursor.execute(&quot;&quot;&quot;
        -- Idempotency for convenience.
        DROP TABLE IF EXISTS denormalised;
        CREATE TABLE denormalised (
            id SERIAL PRIMARY KEY,
            foo varchar,
            bar varchar,
            foobar varchar UNIQUE
        );
        -- Disable autovacuum to not interfere with results.
        ALTER TABLE denormalised SET (autovacuum_enabled = false);

        CREATE INDEX idx_foobar ON denormalised (foobar);
    &quot;&quot;&quot;)


def vacuum_tables(cursor):
    print(&quot;- Vacuuming normalised table...&quot;)
    start_time = time.time()
    cursor.execute(&quot;VACUUM ANALYZE normalised;&quot;)
    duration = time.time() - start_time
    print(f&quot;- Vacuum (normalised) took: {duration:.2f} seconds.&quot;)

    print(&quot;- Vacuuming denormalised table...&quot;)
    start_time = time.time()
    cursor.execute(&quot;VACUUM ANALYZE denormalised;&quot;)
    duration = time.time() - start_time
    print(f&quot;- Vacuum (denormalised) took: {duration:.2f} seconds.&quot;)


def populate_normalised_table(cursor):
    print(&quot;- Populating (normalised) table...&quot;)
    start_time = time.time()
    for batch in range(int(NUM_OF_ROWS / INSERT_ROWS_PER_BATCH)):
        rows = []
        for i in range(INSERT_ROWS_PER_BATCH):
            val = i + (batch*INSERT_ROWS_PER_BATCH)
            rows.append(f&quot;('{val}', '{val}')&quot;)

        values = &quot;, &quot;.join(rows)
        cursor.execute(f&quot;INSERT INTO normalised (foo, bar) VALUES {values};&quot;)
    duration = time.time() - start_time
    print(f&quot;- Populating (normalised) took: {duration:.2f} seconds.&quot;)


def populate_denormalised_table(cursor):
    print(&quot;- Populating (denormalised) table...&quot;)
    start_time = time.time()
    for batch in range(int(NUM_OF_ROWS / INSERT_ROWS_PER_BATCH)):
        rows = []
        for i in range(INSERT_ROWS_PER_BATCH):
            val = i + (batch*INSERT_ROWS_PER_BATCH)
            rows.append(f&quot;('{val}', '{val}', '{val}:{val}')&quot;)

        values = &quot;, &quot;.join(rows)
        cursor.execute(f&quot;INSERT INTO denormalised (foo, bar, foobar) VALUES {values};&quot;)
    duration = time.time() - start_time
    print(f&quot;- Populating (denormalised) took: {duration:.2f} seconds.&quot;)


def benchmark_normalised_queries(cursor, values):
    print(f&quot;- Benchmarking {NUM_OF_QUERIES:,} queries against normalised table...&quot;)
    explain_outputs = []

    for i in range(NUM_OF_QUERIES):
        foo = bar = values[i]
        query = f&quot;SELECT foo, bar FROM normalised where foo='{foo}' and bar ='{bar}';&quot;
        cursor.execute(f&quot;EXPLAIN (ANALYSE, BUFFERS, costs off, summary off) {query}&quot;)
        explain_outputs.append(cursor.fetchall())
    print_explain_info(explain_outputs)

def benchmark_denormalised_queries(cursor, values):
    print(f&quot;- Benchmarking {NUM_OF_QUERIES:,} queries against denormalised table...&quot;)
    explain_outputs = []

    for i in range(NUM_OF_QUERIES):
        foo = bar = values[i]
        query = f&quot;SELECT foobar FROM denormalised where foobar='{foo}:{bar}';&quot;
        cursor.execute(f&quot;EXPLAIN (ANALYSE, BUFFERS, costs off, summary) {query}&quot;)
        explain_outputs.append(cursor.fetchall())
    print_explain_info(explain_outputs)

def benchmark_normalised_queries_foo_only(cursor, values):
    print(f&quot;- Benchmarking {NUM_OF_QUERIES:,} foo-only queries against normalised table...&quot;)
    explain_outputs = []

    for i in range(NUM_OF_QUERIES):
        foo = bar = values[i]
        query = f&quot;SELECT foo, bar FROM normalised where foo='{foo}';&quot;
        cursor.execute(f&quot;EXPLAIN (ANALYSE, BUFFERS, costs off, summary off) {query}&quot;)
        explain_outputs.append(cursor.fetchall())
    print_explain_info(explain_outputs)

def benchmark_denormalised_queries_foo_only(cursor, values):
    print(f&quot;- Benchmarking {NUM_OF_QUERIES:,} foo-only queries against denormalised table...&quot;)
    explain_outputs = []

    for i in range(NUM_OF_QUERIES):
        foo = bar = values[i]
        query = f&quot;SELECT foobar FROM denormalised where foobar LIKE '{foo}:%';&quot;
        cursor.execute(f&quot;EXPLAIN (ANALYSE, BUFFERS, costs off, summary) {query}&quot;)
        explain_outputs.append(cursor.fetchall())
    print_explain_info(explain_outputs)


def print_explain_info(explain_outputs):
    shared_hit = 0
    shared_read = 0
    shared_written = 0
    total_timing = (0, 0)

    for explain_output in explain_outputs:
        found_buffers = False
        for line in explain_output:
            line = line[0]
            if line.startswith(&quot;  Buffers:&quot;) and found_buffers is False:
                found_buffers = True
                # Look for buffer stats in the output using regular expressions
                match = re.search(r&quot;shared hit=(\d+)&quot;, line)
                if match:
                    shared_hit += int(match.group(1))
                match = re.search(r&quot;read=(\d+)&quot;, line)
                if match:
                    shared_read += int(match.group(1))
                match = re.search(r&quot;written=(\d+)&quot;, line)
                if match:
                    shared_written += int(match.group(1))

        timing = explain_output[0][0].split(&quot;actual time=&quot;)[1].split(&quot; &quot;)[0].split(&quot;..&quot;)
        total_timing = (total_timing[0] + float(timing[0]), total_timing[1] + float(timing[1]))

    size = len(explain_outputs)
    print(f&quot;  - average timing = {total_timing[0]/size}..{total_timing[1]/size}&quot;)
    print(f&quot;  - average buffers hit {shared_hit/size}&quot;)
    print(f&quot;  - average buffers read {shared_read/size}&quot;)
    print(f&quot;  - average buffers written {shared_written/size}&quot;)

def run_tests():
    results = {}
    print(
        f&quot;\nReport details:\n&quot;
        f&quot;  - rows in each table: {NUM_OF_ROWS:,}\n&quot;
        f&quot;  - number of queries to benchmark: {NUM_OF_QUERIES:,}\n&quot;
    )
    cursor, conn = get_cursor_and_connection()

    create_normalised_table(cursor)
    populate_normalised_table(cursor)

    create_denormalised_table(cursor)
    populate_denormalised_table(cursor)

    vacuum_tables(cursor)

    query_values = [random.randint(0, NUM_OF_ROWS) for _ in range(NUM_OF_QUERIES)]
    for i in range(2):
        # For a fair measurement, we need to run a second time to see how
        # caching would affect the results.
        benchmark_normalised_queries(cursor, query_values)
        benchmark_denormalised_queries(cursor, query_values)
        benchmark_normalised_queries_foo_only(cursor, query_values)
        benchmark_denormalised_queries_foo_only(cursor, query_values)

    cursor.close()
    conn.close()


if __name__ == &quot;__main__&quot;:
    run_tests()
</code></pre>
<p>Here is the result for the report:</p>
<pre><code>Report details:
  - rows in each table: 10,000,000
  - number of queries to benchmark: 30,000

- Creating normalised table...
- Populating (normalised) table...
- Populating (normalised) took: 136.66 seconds.
- Creating denormalised table...
- Populating (denormalised) table...
- Populating (denormalised) took: 256.72 seconds.
- Vacuuming normalised table...
- Vacuum (normalised) took: 1.37 seconds.
- Vacuuming denormalised table...
- Vacuum (denormalised) took: 1.61 seconds.
- Benchmarking 30,000 queries against normalised table...
  - average timing = 0.08297973333333454..0.08315866666666841
  - average buffers hit 3.1326666666666667
  - average buffers read 0.8673333333333333
  - average buffers written 0.3596
- Benchmarking 30,000 queries against denormalised table...
  - average timing = 0.09937203333333515..0.09953386666666716
  - average buffers hit 4.157966666666667
  - average buffers read 0.8420333333333333
  - average buffers written 0.03643333333333333
- Benchmarking 30,000 foo-only queries against normalised table...
  - average timing = 0.031196766666643467..0.03136039999997724
  - average buffers hit 3.1750333333333334
  - average buffers read 0.8249666666666666
  - average buffers written 0.0
- Benchmarking 30,000 foo-only queries against denormalised table...
  - average timing = 373.48803086666567..448.2494357000009
  - average buffers hit 16117.526433333333
  - average buffers read 66226.47356666667
  - average buffers written 0.0
- Benchmarking 30,000 queries against normalised table...
  - average timing = 0.04304369999998034..0.04320683333331343
  - average buffers hit 3.1661
  - average buffers read 0.8339
  - average buffers written 0.0
- Benchmarking 30,000 queries against denormalised table...
  - average timing = 0.08817753333332631..0.08833266666665922
  - average buffers hit 4.155433333333334
  - average buffers read 0.8445666666666667
  - average buffers written 0.0
- Benchmarking 30,000 foo-only queries against normalised table...
  - average timing = 0.018202833333331735..0.018360799999997398
  - average buffers hit 3.1735333333333333
  - average buffers read 0.8264666666666667
  - average buffers written 0.0
- Benchmarking 30,000 foo-only queries against denormalised table...
  - average timing = 350.0887793666639..421.00194770000036
  - average buffers hit 16121.111933333334
  - average buffers read 66222.88806666667
  - average buffers written 0.0
</code></pre>
<p>As you can see, the denormalised version performs better in every case.</p>
</body>
</html>
