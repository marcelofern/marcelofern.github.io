<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <style rel="stylesheet">
    body{
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      margin:40px auto;
      max-width:750px;
      font-size:18px;
      padding:0 10px;
      color: #333;
      text-align: justify;
    }
    pre {
      background: #f4f4f4;
      border: 1px solid #ddd;
      border-left: 3px solid #000;
      color: #777;
      page-break-inside: avoid;
      font-family: monospace;
      font-size: 15px;
      max-width: 100%;
      overflow: auto;
      padding: 1em 1.5em;
      display: block;
      word-wrap: break-word;
    }
    a {
      color: #1976d2;
      text-decoration: none;
      border-bottom: 1px solid;
    }
    img {
      max-width: 100%;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    li {
      margin-bottom: 5px;
    }
    :not(pre) > code {
      background-color: #f4f4f4;
      padding-right: 0.2em;
      padding-left: 0.2em;
      border-radius: 3px;
    }
  </style>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<p>April 15, 2023</p>
<h2>Introduction</h2>
<p>SOLID is an acronym coined by Robert C. Martin (also known as uncle Bob),
particularly focused at making Object Oriented Programming designs easier to
understand, maintain, and adapt.</p>
<p>The <a href="https://web.archive.org/web/20150906155800/http://www.objectmentor.com/resources/articles/Principles_and_Patterns.pdf">original paper (archived)</a>
introducing the term in 2000 is a quick read worth checking, even if only for
historical context.</p>
<p>Before the paper starts to talk about SOLID, it mentions the 4 symptoms of
&quot;rotting software&quot; (a very popular term between 1998-2006 according to
google ngram). Those 4 symptoms are: <strong>rigidity</strong>, <strong>fragility</strong>,
<strong>immobility</strong> and <strong>viscosity</strong>.</p>
<p>It is important to know what those terms mean, because they are the reason that
SOLID principles exist in the first place. Here is a brief summary:</p>
<ul>
<li>Rigidity: How difficult it is to change the code.</li>
<li>Fragility: How easy it is to break the code.</li>
<li>Immobility: How hard it is to reuse existing code.</li>
<li>Viscosity: How hard it is to preserve the existing design of code when
developing new changes.</li>
</ul>
<p>The general expectation set by Robert C. Martin is that if you follow the SOLID
principles your code will experience less software rot.</p>
<p>However, my own expectation for this article is to provide satisfactory
examples showing how SOLID can actually become <strong>the cause</strong> of software rot.</p>
<h2>SOLID</h2>
<p>The 5 principles of object oriented <strong>class</strong> design (as called by the paper),
are:</p>
<ul>
<li><strong>S</strong>ingle responsibility principle.</li>
<li><strong>O</strong>pen-closed principle.</li>
<li><strong>L</strong>iskov substitution principle.</li>
<li><strong>I</strong>nterface segregation principle.</li>
<li><strong>D</strong>ependency inversion principle.</li>
</ul>
<p>My plan is to analyse each of them critically, understanding their weaknesses
and providing evidence to counter their adoption.</p>
<h3>The Open Closed Principle (OPC)</h3>
<p>According to Martin this is the most important principle, and we will start
with it. Here is its definition:</p>
<blockquote>
<p>A module should be open for extension but closed for modification.</p>
</blockquote>
<p>This sounds simple and reasonable. Perfect code that addresses a particular
problem is only needed to be written once. To address further problems, this
perfect code doesn't need to change, but only be <em>extended</em>.</p>
<p>When developers keep this in mind, they try to produce perfect code that can be
extended whilst keeping its original elegance and efficiency. If that's
materialised, the developer can be said to be following the OPC principle.</p>
<p>You probably can see where this is going by my wording (&quot;perfect code&quot;,
whatever that means). But let's not diverge from the theme just yet, we will go
through an example of code provided by Martin that violates the OPC principle:</p>
<pre><code class="language-cpp">struct Modem {
    enum Type {hayes, courrier, ernie) type;
};

struct Hayes {
    Modem::Type type;
    // Hayes related stuff
};

struct Courrier {
    Modem::Type type;
    // Courrier related stuff
};

struct Ernie
{
    Modem::Type type;
    // Ernie related stuff
};

void LogOn(Modem&amp; m, string&amp; pno, string&amp; user, string&amp; pw) {
    if (m.type == Modem::hayes)
        DialHayes((Hayes&amp;)m, pno);
    else if (m.type == Modem::courrier)
        DialCourrier((Courrier&amp;)m, pno);
    else if (m.type == Modem::ernie)
        DialErnie((Ernie&amp;)m, pno)
    // ...you get the idea
}
</code></pre>
<p>The <code>LogOn</code> function violates OPC because its inner code needs to change every
time a new modem is added. Moreover, every modem type depends on the <code>struct Modem</code>. Therefore if we need to add a new type of modem to that struct, all the
existing modems need to be recompiled.</p>
<p>So how do we make this code better? According to Martin:</p>
<blockquote>
<p>Abstraction is the key to the OCP</p>
</blockquote>
<p>There're a few possible abstraction techniques that conform to OPC. Let's
follow the first one Martin provides, <strong>Dynamic Polymorphism</strong>.</p>
<h4>Dynamic Polymorphism</h4>
<p>First of all, poly means <em>many</em> and morphism means <em>forms</em>. Many forms. In
Object Oriented Programming this means we will have an abstract class with
abstract methods (virtual functions), and concrete child classes implementing
the actual code for each method.</p>
<p>Now we add the word &quot;Dynamic&quot; in the bake, thus <strong>Dynamic Polymorphism</strong>. Here
dynamic means that the form (concrete class implementation) is found during run
time. Putting it all together and rewriting the code above, we end up with
something like the example provided in Martin's paper:</p>
<pre><code class="language-cpp">class Modem {
public:
    virtual void Dial(const string&amp; pno) = 0;
    // other virtual methods here, you get the idea.
};

void LogOn(Modem&amp; m, string&amp; pno, string&amp; user, string&amp; pw) {
    m.Dial(pno);
    // you get the idea.
}
</code></pre>
<p>Now the class <code>Modem</code> is closed for modification. To use the LogOn function,
you need code like this:</p>
<pre><code class="language-cpp">class Hayes : public Modem {
public:
    virtual void Dial(const std::string&amp; pno) {
        // do something...
    }
};


int main() {
    Hayes hayes = Hayes();
    string pno = &quot;1&quot;;
    LogOn(hayes, pno);
}
</code></pre>
<p>Now that we see the whole picture we can tell that the function <code>LogOn</code> calls
the method <code>Dial</code> on the child instance. This child instance is only
available at runtime. Note that Martin omits this initialisation code in his
original paper. He also does not discuss the downsides of this approach.
However, I don't think we should toss it away too quickly. Let's critique the
example a little bit before moving on.</p>
<p>We know for a fact that when types are only resolved at runtime our code
becomes slower. But why? For each class that inherits virtual functions the
compiler creates a <code>vtable</code>. A <code>vtable</code> is essentially a table of function
pointers. When an object is created at runtime, a pointer to its vtable is
added to its memory layout, and when a virtual function is called on this
object, the code looks at the appropriate function pointer on the object vtable
and then calls the function through that pointer.</p>
<p>This all means that by using <strong>Dynamic polymorphism</strong> our code requires
an additional level of indirection for <em>each</em> virtual function call, and the
memory footprint of objects will increase program memory usage.</p>
<p>Furthermore, the given example is a simplistic one. In cases where dynamic
polymorphism is overused, the code may have convoluted inheritance
hierarchies. Such complexity impairs compiler optimization as virtual function
calls aren't assignable until runtime.</p>
<p>You might be thinking that the code above looks clean and that there's no
reason to not do it. After all, reduced performance and more memory usage is
a fair price to pay for cleaner code,  right? Well. What if we didn't have
to pay this price and still have &quot;clean code&quot; (whatever that means)?</p>
<p>We will get there, but to keep things fair we need to go through another
abstraction example given by Martin.</p>
<h4>Static Polymorphism</h4>
<p>What if there was a way to have something similar to Dynamic Polymorphism but
without the runtime overhead, with more compiler optimisation, and more
control over object types? Here is where static polymorphism comes handy.</p>
<p><strong>Static</strong> here means that the child type will be defined at compile time. But
how? The answer is by using generic programming templates.</p>
<pre><code class="language-cpp">template &lt;typename MODEM&gt;
void LogOn(MODEM&amp; m, string&amp; pno, string&amp; user, string&amp; pw) {
    m.Dial(pno);
    // you get the idea.
}
</code></pre>
<p>This looks alright, but is this a real improvement? What is the trade off?</p>
<p>Each time a template is instantiated a new copy of the code is created. This
is how compilers make generic programming type safe at compiling time. The more
instantiations your code has, the larger your executable becomes, and the
longer compilation takes.</p>
<p>This all means that we are trading runtime performance for compile time
performance. It might be OK to do so in certain applications, but what if we
didn't have to?</p>
<h4>Addressing the open-closed principle</h4>
<p>Martin's original argument stressed the following:</p>
<ol>
<li>The dependency between the Modem struct and its implementation structs is bad.</li>
</ol>
<p>My problem with this assertion is that it sounds like a straw man fallacy. You
absolutely don't need to create one struct for each type of modem. Let's fix
the example:</p>
<pre><code class="language-cpp">enum ModemType { HAYES, COURRIER, ERNIE };

struct Modem {
    ModemType type;
    // other attributes, you get the idea.
};

void LogOn(Modem&amp; m, string&amp; pno) {
    switch (m.type) {
        case ModemType.HAYES:
            // do something with hayes
        case ModemType.COURRIER:
            // do something with courrier
        // ...you get the idea
    }
}
</code></pre>
<ol>
<li>Now the compiler knows all the code paths the code goes through at compiling
time. Code can easily be optimised.</li>
<li>No layers of indirection and thus no performance costs at runtime.</li>
<li>No memory overhead.</li>
<li>No executable size overhead.</li>
<li>Less lines of code.</li>
</ol>
<p>Is the code above objectively bad? Is it difficult to read? I could argue that
any CS student could make sense of this code in a very short amount of time.</p>
<p>The idea behind polymorphism is that it makes code flexible and easier to read.
However, the flexibility of using virtual functions can be costly as we have
seen. If you don't know whether you need that flexibility yet, you will be
imagining future use cases that <em>may</em> use your code. If this flexibility ends
up not being needed, the programmer has fundamentally over-scoped their own
code and caused unnecessary memory and CPU degradation to their program with no
added benefit.</p>
<p>Here is where the trade-off lies. Imagine that you need to add a new type of
modem. Using the <code>switch</code> above you'd have to change the LogOn function and
recompile it. You will also need to recompile everything that depends on the
LogOn function, and that can be a lot. If you were using polymorphism, you just
needed to add a new type (potentially in a new file), and you would need to
only compile one single file. Looks good, right?</p>
<p>But what if you need to add a new functionality in the abstract class? In the
polymorphism case you'd need to add a new function for every single type, and
that could be a lot. Differently, in the <code>switch</code> case, you could add a new
function (potentially in a new file) and you only need to compile that one
single file.</p>
<p>In conclusion, this design choice will determine what will be easier and what
will be harder for your program. Ask yourself the question: &quot;Do I want it to be
easier to add more functionality to my program, or do I prioritise adding more
types?&quot;. The answer will dictate what is best on your situation. Polymorphism
isn't a silver bullet that will always make your code OPC compliant. In fact,
the more functional your code looks like, the greater the penalty of using
polymorphism.</p>
<h3>The Liskov Substitution Principle (LSP)</h3>
<p>This principle states that an object (such as an abstract class), may be
replaced by a sub-object (such as a child class) without breaking the program.</p>
<p>This means that if we have a function <code>foo(Parent bar)</code>, we should also be
able to call foo as <code>foo(Child bar)</code> without altering the correctness of the
program.</p>
<p>Martin uses an example of a parent class called <code>Ellipse</code> and a child class
called <code>Circle</code>. As every circle is an ellipse with a very particular
configuration, this sounds about right. However, Martin only uses this example
to stress that it is an inheritance <em>bad</em> practice.</p>
<pre><code class="language-cpp">void f(Ellipse&amp; e) {
    Point a(-1,0);
    Point b(1,0);
    e.SetFoci(a,b);
    e.SetMajorAxis(3);
    assert(e.GetFocusA() == a);
    assert(e.GetFocusB() == b);
    assert(e.GetMajorAxis() == 3);
}
</code></pre>
<p>The function <code>f</code> above, for example, can't receive a <code>Circle</code> instead of an
<code>Ellipse</code>. Reason being that the method <code>setFoci</code> will alter the circle and
turn it into an ellipse. This can become a subtle bug in the application code.</p>
<p>A safe option is for the <code>Circle::SetFoci</code> method to add an extra validation,
asserting that <code>a == b</code>. This violates LSP. The child object now has an extra
restriction that the parent object doesn't. Martin concludes that:</p>
<blockquote>
<p>derived methods should expect no more and provide no less.</p>
</blockquote>
<p>I don't have much to critique about this principle itself. To be honest it
sounds alright to me. However, this is only <strong>one more thing</strong> to remember when
you're using polymorphism, which contributes as a <strong>negative</strong> to the OPC
principle through abstraction that we discussed above.</p>
<p>Apart from performance and memory degradation, the programmer also has to worry
about loose/tight contracts between the parent and the child. This means that
the functionality may not work that well if the type implementation is
faulty.</p>
<p>Martin himself comments that LSP violation can be costly. If the interface is
being used in many different places, the cost of repairing this violation might
be too much to take. A possible solution, as stated by Martin, is to provide an
<code>if/else</code> statement to make sure the Ellipse is indeed an Ellipse.</p>
<p>Another problem of this type of polymorphism is that the Circle object is way
simpler than an Ellipse. This means that the Circle class will inherit many
methods that it doesn't need to use, generating <strong>method spam</strong>. This violates
another principle of SOLID, the Interface Segregation Principle that we will
look into later on.</p>
<h2>The Dependency Inversion Principle (DIP)</h2>
<blockquote>
<p>Depend upon Abstractions. Do not depend upon concretions.</p>
</blockquote>
<p>In essence, this principle means that code in high level modules should not
depend on code in low level modules. High level modules are usually top
abstractions that express the policies of the application, whereas low level
modules contain the implementation details. In other words, the abstraction
should not worry about the implementation.</p>
<p>By inverting the dependency, i.e., making the low level modules depend on the
high ones and not the opposite, a developer will be DIP complaint.</p>
<p>You might be noticing a trend now. Most of these SOLID rules are greatly
related to polymorphism. After all, they were created for OOP. As I feel like I
have addressed the main trade-off in that approach, I could easily end up
repeating myself here. Let's proceed nonetheless :')</p>
<p>Inverting dependencies in a codebase must be seen as a trade-off. As now your
abstraction can be changed without having to worry about the implementation
details, you can't change the implementation details without having to worry
about the abstraction. This means that if the interface changes often, it will
be harder to manage the concrete implementations.</p>
<p>More over, classes that could simply be a concrete implementation alone, with
no dependency on a high-level interface, now may have an artificial interface
so that this principle isn't violated. This usually happens because &quot;you never
know whether another concrete implementation that needs to use the interface
might come about&quot;. One regular example is an interface <code>ShipmentPolicy</code> which
has <strong>only one</strong> concrete implementation, often called <code>ShipmentPolicyImpl</code>.
One could say that this is a code redundancy.</p>
<h2>The Interface Segregation Principle (ISP)</h2>
<p>ISP states that no code should depend on methods it does not use. This implies
having smaller interfaces so that clients that depend on them only need to know
a few relevant methods.</p>
<p>This idea sounds reasonable, and it's a way of controlling inheritance method
spam when inheriting from bloated interfaces. But I think this technique is
more of a refactoring tool rather than a principle itself.</p>
<p>Treating ISP as a principle can add unnecessary complexity. You should design
your code to solve the problem at hand rather than worrying about whether your
interface will become too bloated in future when more use cases are added. As
you don't know how your codebase will progress in future, you shouldn't make
compromises from early on. If an interface grew too much, and <strong>now</strong> you have
legitimate reason to split it into smaller ones, then go and refactor it.</p>
<p>As all the other principles we have discussed so far, take ISP as a
trade-off instead of a principle.</p>
<p>Here are a few quotes from the Code Complete book that are relevant for the
matter:</p>
<blockquote>
<p>If the derived class isn't going to adhere completely to the same interface
contract defined by the base class, inheritance is not the right
implementation.</p>
</blockquote>
<blockquote>
<p>Be suspicious of base classes of which there is only one derived class.</p>
</blockquote>
<h2>Single Responsibility Principle (SRP)</h2>
</body>
</html>
