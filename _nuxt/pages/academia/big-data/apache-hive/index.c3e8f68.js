(window.webpackJsonp=window.webpackJsonp||[]).push([[3],{217:function(e,t,n){"use strict";n(111);var r=n(74);t.a=Object(r.a)("layout")},218:function(e,t,n){"use strict";n(26),n(12);var r=n(3),o=(n(46),n(209),n(47),n(6),n(4),n(17),n(58),n(59),n(145),n(0)),c=n(112),l=n(2);function d(object,e){var t=Object.keys(object);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(object);e&&(n=n.filter((function(e){return Object.getOwnPropertyDescriptor(object,e).enumerable}))),t.push.apply(t,n)}return t}function v(e){for(var i=1;i<arguments.length;i++){var source=null!=arguments[i]?arguments[i]:{};i%2?d(Object(source),!0).forEach((function(t){Object(r.a)(e,t,source[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(source)):d(Object(source)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(source,t))}))}return e}var f=["sm","md","lg","xl"],h=["start","end","center"];function w(e,t){return f.reduce((function(n,r){return n[e+Object(l.n)(r)]=t(),n}),{})}var m=function(e){return[].concat(h,["baseline","stretch"]).includes(e)},y=w("align",(function(){return{type:String,default:null,validator:m}})),_=function(e){return[].concat(h,["space-between","space-around"]).includes(e)},H=w("justify",(function(){return{type:String,default:null,validator:_}})),O=function(e){return[].concat(h,["space-between","space-around","stretch"]).includes(e)},C=w("alignContent",(function(){return{type:String,default:null,validator:O}})),j={align:Object.keys(y),justify:Object.keys(H),alignContent:Object.keys(C)},S={align:"align",justify:"justify",alignContent:"align-content"};function D(e,t,n){var r=S[e];if(null!=n){if(t){var o=t.replace(e,"");r+="-".concat(o)}return(r+="-".concat(n)).toLowerCase()}}var P=new Map;t.a=o.a.extend({name:"v-row",functional:!0,props:v(v(v({tag:{type:String,default:"div"},dense:Boolean,noGutters:Boolean,align:{type:String,default:null,validator:m}},y),{},{justify:{type:String,default:null,validator:_}},H),{},{alignContent:{type:String,default:null,validator:O}},C),render:function(e,t){var n=t.props,data=t.data,o=t.children,l="";for(var d in n)l+=String(n[d]);var v=P.get(l);return v||function(){var e,t;for(t in v=[],j)j[t].forEach((function(e){var r=n[e],o=D(t,e,r);o&&v.push(o)}));v.push((e={"no-gutters":n.noGutters,"row--dense":n.dense},Object(r.a)(e,"align-".concat(n.align),n.align),Object(r.a)(e,"justify-".concat(n.justify),n.justify),Object(r.a)(e,"align-content-".concat(n.alignContent),n.alignContent),e)),P.set(l,v)}(),e(n.tag,Object(c.a)(data,{staticClass:"row",class:v}),o)}})},239:function(e,t,n){"use strict";n.r(t);var r={data:function(){return{hcatalogUrl:"/images/big_data/Hcatalog.png"}},head:{title:"Apache Hive - MarceloFern",meta:[{hid:"description",name:"description",content:"Apache Hive - Marcelo Fernandes"}]}},o=n(53),c=n(73),l=n.n(c),d=n(216),v=n(217),f=n(218),component=Object(o.a)(r,(function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("v-layout",{staticClass:"post"},[n("v-flex",[n("v-row",[n("h1",[e._v("Apache Hive")])]),e._v(" "),n("v-row",[n("p",{staticClass:"caption"},[e._v("Date: 2020-11-19")])]),e._v(" "),n("v-row",[n("p",[e._v("\n        The Apache Hive data warehouse software facilitates\n        reading, writing, and managing of large datasets\n        that reside in distributed storages.\n      ")]),e._v(" "),n("p",[e._v("\n        Hive is built on top of Hadoop, and it provides\n        the following features:\n      ")])]),e._v(" "),n("v-row",[n("ul",[n("li",[e._v("\n          Tools that enable easy access to data via SQL,\n          facilitating data warehousing tasks such as\n          extract/transform/load, reporting, and data analysis.\n        ")]),e._v(" "),n("li",[e._v("\n          A mechanism to impose structure on a variety of data\n          formats.\n        ")]),e._v(" "),n("li",[e._v("\n          Access to files that are either stored directly in HDFS\n          or in other data storage system such as HBase.\n        ")]),e._v(" "),n("li",[e._v("\n          Query execution via Tez, Spark, or MapReduce.\n        ")]),e._v(" "),n("li",[e._v("\n          Procedural language with HPL-SQL\n        ")]),e._v(" "),n("li",[e._v("\n          Sub-second query retrieval via Hive LLAP, YARN, and\n          Apache Slider.\n        ")])])]),e._v(" "),n("v-row",[n("p",[e._v("\n      Hive provides standard SQL functionality. Hive's SQL can also be\n      extended with user code via user defined functions (UDFs), user\n      defined aggregates (UDAFs), and user defined table functions (UDTFs).\n      ")])]),e._v(" "),n("v-row",[n("p",[e._v('\n        There is not a single "Hive format" in which data must be stored.\n        Hive comes with many built in connectors for comma and tab-separated\n        values (CSV/TSV) text files, Apache Parquet, Apache ORC, and other\n        formats. Users can extend Hive with connectors for other formats.\n      ')]),e._v(" "),n("p",[e._v("\n        Hive is not designed for online transaction processing (OLTP)\n        workloads. Instead, it is best used for traditional data warehousing\n        tasks.\n      ")]),e._v(" "),n("p",[e._v("\n        Hive is designed to maximise scalability (scale out with more\n        machines added dynamically to Hadoop cluster), performance,\n        extensibility, fault-tolerance, and loose-coupling with its\n        input formats.\n      ")])]),e._v(" "),n("v-row",[n("h2",[e._v("HCatalog")])]),e._v(" "),n("v-row",[n("p",[e._v("\n        HCatalog is a table and storage management layer for Hadoop that\n        enables users with different data processing tools (Pig, MapReduce)\n        to more easily read and write data on the grid. HCatalog's\n        table abstraction presents users with a relational view of data\n        inside HDFS, and ensures that users do not need to worry\n        about where or in what format their data is stored (RCFile, text,\n        SequenceFiles, or ORC files).\n      ")]),e._v(" "),n("p",[e._v("\n        HCatalog supports reading and writing files in any format\n        for which a SerDe (serializer-deserializer) can be written.\n        By default, HCatalog supports RCFile, CSV, JSON, SequenceFile,\n        and ORC file formats. To use a custom format, you must\n        provide the InputFormat, OutputFormat, and SerDe.\n      ")])]),e._v(" "),n("v-row",[n("a",{attrs:{href:e.hcatalogUrl,target:"_blank"}},[n("img",{attrs:{src:e.hcatalogUrl}})])]),e._v(" "),n("v-row",[n("p",{staticClass:"small-text"},[n("i",[e._v("Click on the image to expand it in a new tab.")])])]),e._v(" "),n("v-row",[n("p",[e._v("\n        HCatalog is built on top of the Hive metastore and incorporates\n        Hive's DDL. HCatalog provides read and write interfaces for Pig\n        and MapReduce and uses Hive's command line interface for issuing\n        data definition and metadata exploration commands.\n      ")])]),e._v(" "),n("v-row",[n("h2",[e._v("WebHCat")])]),e._v(" "),n("v-row",[n("p",[e._v("\n        WebHCat is the HCatalog REST API. Developers make HTTP requests\n        to access Hadoop MapReduce (or YARN), Pig, Hive, and HCatalog\n        DDL from within applications. Data and code used by this API are\n        maintained in HDFS. HCatalog DDL commands are executed\n        directly when requested. MapReduce, Pig, and Hive jobs are placed\n        in queue by WebHCat servers and can be monitored for progress\n        or stopped as required. Developers specify a location in HDFS into\n        which Pig, Hive, and MapReduce results should be placed.\n      ")])])],1)],1)}),[],!1,null,"1c1c7408",null);t.default=component.exports;l()(component,{VFlex:d.a,VLayout:v.a,VRow:f.a})}}]);