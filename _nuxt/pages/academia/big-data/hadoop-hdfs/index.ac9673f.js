(window.webpackJsonp=window.webpackJsonp||[]).push([[5],{217:function(e,t,n){"use strict";n(111);var o=n(74);t.a=Object(o.a)("layout")},218:function(e,t,n){"use strict";n(26),n(12);var o=n(3),r=(n(46),n(209),n(47),n(6),n(4),n(17),n(58),n(59),n(145),n(0)),c=n(112),l=n(2);function d(object,e){var t=Object.keys(object);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(object);e&&(n=n.filter((function(e){return Object.getOwnPropertyDescriptor(object,e).enumerable}))),t.push.apply(t,n)}return t}function h(e){for(var i=1;i<arguments.length;i++){var source=null!=arguments[i]?arguments[i]:{};i%2?d(Object(source),!0).forEach((function(t){Object(o.a)(e,t,source[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(source)):d(Object(source)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(source,t))}))}return e}var f=["sm","md","lg","xl"],m=["start","end","center"];function v(e,t){return f.reduce((function(n,o){return n[e+Object(l.n)(o)]=t(),n}),{})}var y=function(e){return[].concat(m,["baseline","stretch"]).includes(e)},w=v("align",(function(){return{type:String,default:null,validator:y}})),_=function(e){return[].concat(m,["space-between","space-around"]).includes(e)},k=v("justify",(function(){return{type:String,default:null,validator:_}})),D=function(e){return[].concat(m,["space-between","space-around","stretch"]).includes(e)},N=v("alignContent",(function(){return{type:String,default:null,validator:D}})),H={align:Object.keys(w),justify:Object.keys(k),alignContent:Object.keys(N)},S={align:"align",justify:"justify",alignContent:"align-content"};function j(e,t,n){var o=S[e];if(null!=n){if(t){var r=t.replace(e,"");o+="-".concat(r)}return(o+="-".concat(n)).toLowerCase()}}var F=new Map;t.a=r.a.extend({name:"v-row",functional:!0,props:h(h(h({tag:{type:String,default:"div"},dense:Boolean,noGutters:Boolean,align:{type:String,default:null,validator:y}},w),{},{justify:{type:String,default:null,validator:_}},k),{},{alignContent:{type:String,default:null,validator:D}},N),render:function(e,t){var n=t.props,data=t.data,r=t.children,l="";for(var d in n)l+=String(n[d]);var h=F.get(l);return h||function(){var e,t;for(t in h=[],H)H[t].forEach((function(e){var o=n[e],r=j(t,e,o);r&&h.push(r)}));h.push((e={"no-gutters":n.noGutters,"row--dense":n.dense},Object(o.a)(e,"align-".concat(n.align),n.align),Object(o.a)(e,"justify-".concat(n.justify),n.justify),Object(o.a)(e,"align-content-".concat(n.alignContent),n.alignContent),e)),F.set(l,h)}(),e(n.tag,Object(c.a)(data,{staticClass:"row",class:h}),r)}})},241:function(e,t,n){"use strict";n.r(t);var o={data:function(){return{hadoopDocUrl:"https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#Introduction",hdfsUrl:"/images/big_data/hdfs.png",replicaUrl:"/images/big_data/data_nodes_replicas.png",userQuotasUrl:"https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsQuotaAdminGuide.html",accessPermissionsUrl:"https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.html",rackAwareUrl:"https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/RackAwareness.html"}},head:{title:"Hadoop - HDFS - MarceloFern",meta:[{hid:"description",name:"description",content:"Hadoop - HDFS- Marcelo Fernandes"}]}},r=n(53),c=n(73),l=n.n(c),d=n(216),h=n(217),f=n(218),component=Object(r.a)(o,(function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("v-layout",{staticClass:"post"},[n("v-flex",[n("v-row",[n("h1",[e._v("Hadoop - HDFS")])]),e._v(" "),n("v-row",[n("p",{staticClass:"caption"},[e._v("Date: 2020-11-16")])]),e._v(" "),n("v-row",[n("p",[e._v("\n        According to Hadoop's documentation\n        "),n("a",{attrs:{href:e.hadoopDocUrl,target:"_blank"}},[e._v("[1]")]),e._v(", the\n        Hadoop Distributed File System (HDFS) is a distributed\n        file system designed to run on commodity (cheap) hardware.\n      ")]),e._v(" "),n("p",[e._v("\n        It is manly made to be highly fault-tolerant and to be\n        deployed on low-cost hardware.\n      ")])]),e._v(" "),n("v-row",[n("h2",[e._v("Principles")])]),e._v(" "),n("v-row",[n("p",[e._v("\n        HDFS architecture is composed by 6 principles:\n      ")])]),e._v(" "),n("v-row",[n("ol",[n("li",[n("b",[e._v("Hardware Failure")]),e._v(": Hardware failures are always present in large\n          systems. In a HDFS instance\n          that is composed by thousands of machines, some machines\n          are likely to be non-functional at any given time. With that in\n          mind, Hadoop must quickly detect faults and immediately\n          recovery from them.\n        ")]),e._v(" "),n("li",[n("b",[e._v("Streaming Data Access")]),e._v(": HDFS is designed and optimised to perform\n          batch processing rather than interactive use by users.\n          This means that the it is more important to have high throughput\n          of data access rather than low latency of data access.\n        ")]),e._v(" "),n("li",[n("b",[e._v("Large Datasets")]),e._v(": HDFS is optimised to work on large files.\n          It should provide high aggregate data bandwidth and scale to\n          hundreds of nodes in a single cluster. Also, a single instance\n          should support tens of millions of files -- considering that\n          the typical file is gigabytes to terabytes in size.\n        ")]),e._v(" "),n("li",[n("b",[e._v("Simple Coherency Model")]),e._v(": As in most big data frameworks,\n          HDFS applications typically write the data once and then\n          proceed to read it many times. A file that has been created\n          won't be changed except to append or truncate data. Appending\n          the content to the end of the files is supported but cannot\n          be updated at an arbitrary point. This simplifies data coherency\n          issues and enables high throughput access. MapReduce and web-crawler\n          applications fit within this model.\n        ")]),e._v(" "),n("li",[n("b",[e._v('"Moving Computation is Cheaper than Moving Data"')]),e._v(": Computational\n          operations are usually more efficient if they are executed\n          close to the data they are operating on. This is easier to\n          notice when the size of the data is massive. This will minimise\n          network congestion and will increase the overall throughput.\n        ")]),e._v(" "),n("li",[n("b",[e._v("Portability Across Heterogeneous Hardware and Software Platforms")]),e._v(":\n          HDFS should be easy to port from one platform to another.\n        ")])])]),e._v(" "),n("v-row",[n("h2",[e._v("\n        NameNode and DataNodes\n      ")])]),e._v(" "),n("v-row",[n("p",[e._v("\n        HDFS is based on a master/worker architecture.\n        There can be only one master server, which is referred as\n        "),n("b",[e._v("NameNode")]),e._v(". The NameNode will manage the\n        "),n("a",{attrs:{href:"#namespace"}},[e._v("file system namespace")]),e._v(" and control access\n        to files by clients. The "),n("b",[e._v("DataNodes")]),e._v(" manage storage\n        attached to the nodes they run on. When the clients uploads\n        a file, the file will be split into one\n        or more blocks. These blocks are stored in a set of DataNodes.\n      ")]),e._v(" "),n("p",[e._v("\n        The NameNode executes file system namespace operations such as\n        opening, closing, and renaming files and directories. It also\n        determines the mapping of blocks to DataNodes. The NameNode is\n        the arbitrator and repository for all HDFS metadata. Although this\n        greatly facilitates the architecture, it also creates a single point\n        of failure.\n      ")]),e._v(" "),n("p",[e._v("\n        The DataNodes\n        are responsible for serving the read and write requests from\n        the file system's clients. DataNodes also execute block creation,\n        deletion, and replication upon instructions from the NameNode.\n        In a real deployment there will be, usually, one DataNode running\n        per machine, though this is not a constraint.\n      ")])]),e._v(" "),n("v-row",[n("a",{attrs:{href:e.hdfsUrl,target:"_blank"}},[n("img",{attrs:{src:e.hdfsUrl}})])]),e._v(" "),n("v-row",[n("p",{staticClass:"small-text"},[n("i",[e._v("Click on the image to expand it in a new tab.")])])]),e._v(" "),n("v-row",[n("h2",{attrs:{id:"namespace"}},[e._v("\n        File System Namespace\n      ")])]),e._v(" "),n("v-row",[n("p",[e._v("\n        HDFS supports the classic hierarchical file organisation.\n        A client can create directories and store files inside\n        these directories. The file system namespace hierarchy is\n        very similar to other existing file systems, and\n        operations such create, remove, move, and rename files are\n        supported.\n      ")]),e._v(" "),n("p",[e._v("\n        HDFS supports user quotas\n        "),n("a",{attrs:{target:"_blank",href:e.userQuotasUrl}},[e._v("[2]")]),e._v("\n        and access permissions\n        "),n("a",{attrs:{target:"_blank",href:e.accessPermissionsUrl}},[e._v("[3]")]),e._v(".\n        HDFS does not support hard links or soft links.\n      ")]),e._v(" "),n("p",[e._v("\n        The NameNode will maintain the file system namespace.\n        This means that any change to the file system namespace or its\n        properties is recorded by the NameNode. An application can\n        specify the number of replicas of a file that should be\n        maintained by HDFS. The number of copies of a file is called\n        the replication factor of that file, and this information is\n        stored by the NameNode.\n      ")])]),e._v(" "),n("v-row",[n("h2",[e._v("Data Replication")])]),e._v(" "),n("v-row",[n("p",[e._v("\n        HDFS is expected to store large files across many machines\n        in a large cluster. Each file is stored as sequence of blocks,\n        and these blocks are replicated to enhance fault tolerance.\n        Both the block size and replication factor are configurable\n        in a per-file basis. Additionally, the replication factor can be\n        changed after creation.\n      ")]),e._v(" "),n("p",[e._v("\n        All the blocks in a file, except the last block, have the same size.\n        Users can start new blocks without filling out the last block.\n      ")]),e._v(" "),n("p",[e._v("\n        Files in HDFS are "),n("b",[e._v("write-once")]),e._v(" (except for appends and\n        truncates), and are properly locked so that only one writer\n        can operate at any time.\n      ")]),e._v(" "),n("p",[e._v("\n        The NameNode makes all decisions regarding replication of blocks.\n        It periodically receives a Heartbeat and a Blockreport from\n        each of the DataNodes in the cluster. Receipt of a Heartbeat implies\n        that the DataNode is functioning as expected. A Blockreport\n        contains a list the blocks in that DataNode.\n      ")])]),e._v(" "),n("v-row",[n("a",{attrs:{href:e.replicaUrl,target:"_blank"}},[n("img",{attrs:{src:e.replicaUrl}})])]),e._v(" "),n("v-row",[n("p",{staticClass:"small-text"},[n("i",[e._v("Click on the image to expand it in a new tab.")])])]),e._v(" "),n("v-row",[n("h2",[e._v("Replica Placement")])]),e._v(" "),n("v-row",[n("p",[e._v("\n        The optimisation of replica placements is one of the features\n        that distinguishes HDFS from most other distributed file systems.\n        The purpose of the proposed rack-aware replica placement policy\n        is to improve data reliability, availability, and network bandwidth\n        utilisation. The current implementation for the replica placement\n        policy is a first effort in this direction.\n      ")]),e._v(" "),n("p",[e._v("\n        Large HDFS instances typically run on a cluster of computers\n        that are commonly spread across many racks. Communication that\n        happens between two nodes placed in different racks has to go\n        through switches. In typical scenarios, network bandwidth between\n        machines in the same rack is greater than between machines\n        in different racks.\n      ")]),e._v(" "),n("p",[e._v("\n        The NameNode will determine the rack id each DataNode belongs to\n        via a process described in Hadoop Rack Awareness\n        "),n("a",{attrs:{target:"_blank",href:e.rackAwareUrl}},[e._v("[4]")]),e._v(".\n        A simple non-optimal policy is to place replicas on different racks.\n        This prevents losing data when an entire rack fails and allows use\n        of bandwidth from multiple racks when reading data.\n        This policy also evenly distributes replicas in the cluster, which\n        facilitates load balancing on component failure. One of the\n        down-sides is that the cost of writes is increased due to\n        the needs to transfer blocks to multiple racks.\n      ")]),e._v(" "),n("p",[e._v("\n        For the common case, for a replication factor of three, HDFS's\n        placement policy is to put one replica on the local machine\n        if the writer is on a DataNode, otherwise on a random DataNode in\n        the same rack as that of the writer. The second replica would be\n        placed on a node in a different (remote) rack, and the last on\n        a different node in the same remote rack. This policy aids to cut\n        the inter-rack write traffic, which generally improves write\n        performance. The chance of rack failure is far less than that\n        of node failure; therefore this policy does not impact data \n        reliability and availability guarantees. However, the down-side\n        is that it does not reduce the aggregate network bandwidth used\n        when reading data because a block is placed in only two unique\n        racks rather than three. The blocks do not evenly distribute\n        accross the racks. This policy improves write performance\n        without compromising data reliability or read performance too much.\n      ")]),e._v(" "),n("p",[e._v("\n        For a replication factor greater than 3, the 4th and subsequent\n        replicas are determined randomly while keeping the number of\n        replicas per rack below the upper limit, which is basically\n        "),n("i",[e._v("(replicas -1) / racks +2")]),e._v(".\n      ")]),e._v(" "),n("p",[e._v("\n        Since the NameNod does not allow DataNodes to have multiple\n        replicas of the same block, the maximum number of replicas\n        created is the total number of DataNodes at that time.\n      ")]),e._v(" "),n("p",[e._v("\n        The NameNode chooses nodes based on rack awareness at first,\n        and then proceeds to check that the candidate node has\n        storage required by the policy associated with the file.\n        If the candidate node does not have the storage type, the NameNode\n        proceeds to look for another candidate. If no nodes to place\n        replicas can be found, the NameNode looks for nodes having\n        fallback storage types in the second turn.\n      ")])])],1)],1)}),[],!1,null,"09987c5b",null);t.default=component.exports;l()(component,{VFlex:d.a,VLayout:h.a,VRow:f.a})}}]);