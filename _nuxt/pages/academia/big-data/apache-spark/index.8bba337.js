(window.webpackJsonp=window.webpackJsonp||[]).push([[4],{217:function(e,t,n){"use strict";n(111);var r=n(74);t.a=Object(r.a)("layout")},218:function(e,t,n){"use strict";n(26),n(12);var r=n(3),o=(n(46),n(209),n(47),n(6),n(4),n(17),n(58),n(59),n(145),n(0)),c=n(112),l=n(2);function d(object,e){var t=Object.keys(object);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(object);e&&(n=n.filter((function(e){return Object.getOwnPropertyDescriptor(object,e).enumerable}))),t.push.apply(t,n)}return t}function h(e){for(var i=1;i<arguments.length;i++){var source=null!=arguments[i]?arguments[i]:{};i%2?d(Object(source),!0).forEach((function(t){Object(r.a)(e,t,source[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(source)):d(Object(source)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(source,t))}))}return e}var f=["sm","md","lg","xl"],m=["start","end","center"];function v(e,t){return f.reduce((function(n,r){return n[e+Object(l.n)(r)]=t(),n}),{})}var y=function(e){return[].concat(m,["baseline","stretch"]).includes(e)},S=v("align",(function(){return{type:String,default:null,validator:y}})),w=function(e){return[].concat(m,["space-between","space-around"]).includes(e)},k=v("justify",(function(){return{type:String,default:null,validator:w}})),j=function(e){return[].concat(m,["space-between","space-around","stretch"]).includes(e)},O=v("alignContent",(function(){return{type:String,default:null,validator:j}})),D={align:Object.keys(S),justify:Object.keys(k),alignContent:Object.keys(O)},_={align:"align",justify:"justify",alignContent:"align-content"};function R(e,t,n){var r=_[e];if(null!=n){if(t){var o=t.replace(e,"");r+="-".concat(o)}return(r+="-".concat(n)).toLowerCase()}}var C=new Map;t.a=o.a.extend({name:"v-row",functional:!0,props:h(h(h({tag:{type:String,default:"div"},dense:Boolean,noGutters:Boolean,align:{type:String,default:null,validator:y}},S),{},{justify:{type:String,default:null,validator:w}},k),{},{alignContent:{type:String,default:null,validator:j}},O),render:function(e,t){var n=t.props,data=t.data,o=t.children,l="";for(var d in n)l+=String(n[d]);var h=C.get(l);return h||function(){var e,t;for(t in h=[],D)D[t].forEach((function(e){var r=n[e],o=R(t,e,r);o&&h.push(o)}));h.push((e={"no-gutters":n.noGutters,"row--dense":n.dense},Object(r.a)(e,"align-".concat(n.align),n.align),Object(r.a)(e,"justify-".concat(n.justify),n.justify),Object(r.a)(e,"align-content-".concat(n.alignContent),n.alignContent),e)),C.set(l,h)}(),e(n.tag,Object(c.a)(data,{staticClass:"row",class:h}),o)}})},240:function(e,t,n){"use strict";n.r(t);var r={data:function(){return{hcatalogUrl:"/images/big_data/Hcatalog.png"}},head:{title:"Apache Spark - MarceloFern",meta:[{hid:"description",name:"description",content:"Apache Spark - Marcelo Fernandes"}]}},o=n(53),c=n(73),l=n.n(c),d=n(216),h=n(217),f=n(218),component=Object(o.a)(r,(function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("v-layout",{staticClass:"post"},[n("v-flex",[n("v-row",[n("h1",[e._v("Apache Spark")])]),e._v(" "),n("v-row",[n("p",{staticClass:"caption"},[e._v("Date: 2020-11-20")])]),e._v(" "),n("v-row",[n("p",[e._v("\n        Spark is a unified analytics engine for large-scale data\n        processing. It has high-level API's in multiple languages,\n        including Java, Scala, Python, and R. Spark has an opitmised\n        engine that supports general execution graphs. It als supports\n        a set of higher-level tools such as Spark SQL for SQL and\n        structured data processing, MLlib for machine learning, GraphX\n        for graph processing, and Structured Steaming for incremental\n        computation and stream processing.\n      ")]),e._v(" "),n("p",[e._v("\n        Spark uses Hadoop's client libraries for HDFS and YARN.\n        Python users can install Spark from PyPI.\n      ")])]),e._v(" "),n("v-row",[n("p",[e._v("\n        Apache Spark has its architectural foundation in the resilient\n        dataset (RDD), a read-only multiset of data items distributed\n        over a cluster of machines, that is maintained in a fault-tolerant\n        way.\n      ")]),e._v(" "),n("p",[e._v("\n        Spark and its RDDs were developed in 2012 in response to limitations\n        in the MapReduce cluster computing paradigm, which forces a particular\n        linear data-flow structure on distributed programs: MapReduce programs\n        read input data from disk, map a function across the data, reduce\n        the results of the map, and store reduction results on disk.\n        Spark's RDDs function as a working set for distributed programs\n        that offers a (deliberately) restricted form of distributed\n        shared memory.\n      ")]),e._v(" "),n("p",[e._v("\n        Spark facilitates the implementation of both iterative algorithms,\n        which visit their data set multiple times in a loop, and interactive/\n        exploratory data analysis, i.e., the repeated database-style\n        querying of data. The latency of such applications may be reduced\n        by several orders of magnitude compared to Apache Hadoop MapReduce\n        implementation. Among the class of iterative algorithms are the\n        training algorithms for machine learning systems, which formed\n        the initial impetus for developing Apache Spark.\n      ")]),e._v(" "),n("p",[e._v("\n        Apache Spark requires a cluster manager and a distributed storage\n        system. For cluster management, Spark supports standalone (native\n        Spark cluster, where you can launch a cluster either manually or\n        use the launch scripts provided by the install package. It is also\n        possible to run these daemons on a single machine for testing),\n        Hadoop YARN, Apache Mesos or Kubernetes. For distributed storage,\n        Spark can interface with a wide variety, including Alluxio,\n        HDFS, MapR File System, Cassandra, OpenStack Swift, Amazon S3,\n        Kudu, Lustre file system, or a custom solution can be implemented.\n        Spark also supports a pseudo-distributed local mode, usually\n        used only for development or testing purposes, where distributed\n        storage is not required and the local file system can be used instead;\n        in such a scenario, Spark is run on a single machine with one\n        executor per CPU core.\n      ")])]),e._v(" "),n("v-row",[n("h2",[e._v("Spark Core")])]),e._v(" "),n("v-row",[n("p",[e._v('\n        Spark Core is the basis of the whole project. It provides\n        distributed task dispatching, scheduling, and basic I/O\n        functionalities. The Spark Core is exposed through application\n        programming interfaces (Java, Python, Scala, .NET, and R) centered\n        on the RDD abstraction. This interface mirrors a function/high-order\n        model of programming: a "driver" program invokes parallel operations\n        such as map, filter, or reduce on an RDD by passing a function to Spark,\n        which then schedules the functions\'s execution in parallel to the cluster.\n        These operations, and additional ones such as joins, take RDDs as input\n        and procue new RDDs. RDDs are immutable and their operations\n        are lazy; fault-tolerance is achieved by keeping track of the lineage\n        of each RDD (the sequence of operations that produced it) so that\n        it can be reconstructed in the case of data loss.\n      ')])])],1)],1)}),[],!1,null,"c92a6ea2",null);t.default=component.exports;l()(component,{VFlex:d.a,VLayout:h.a,VRow:f.a})}}]);