<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <style rel="stylesheet">
    body{
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      margin:40px auto;
      max-width:750px;
      font-size:18px;
      padding:0 10px;
      color: #333;
    }
    pre {
      background: #f4f4f4;
      border: 1px solid #ddd;
      border-left: 3px solid #000;
      color: #777;
      page-break-inside: avoid;
      font-family: monospace;
      font-size: 15px;
      max-width: 100%;
      overflow: auto;
      padding: 1em 1.5em;
      display: block;
      word-wrap: break-word;
    }
    a {
      color: #1976d2;
      text-decoration: none;
      border-bottom: 1px solid;
    }
    img {
      max-width: 100%;
    }
    h1 {
      text-align: left;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    li {
      margin-bottom: 10px;
    }
    :not(pre) > code {
      background-color: #f4f4f4;
      padding-right: 0.2em;
      padding-left: 0.2em;
      border-radius: 3px;
    }
  </style>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h1>pg_repack</h1>
<pre><code>Created at: 2024-11-04
</code></pre>
<p>This extension helps with removing bloat from tables and indexes. It can also
optionally restore the physical order of clustered indexes.</p>
<p>Differently from VACUUM FULL, and CLUSTER this works without an exclusive lock
on the table. This extension claims to perform as well as CLUSTER itself.</p>
<p>Here is what it is capable of doing:</p>
<ul>
<li>CLUSTER without exclusive lock (ordered by cluster index).</li>
<li>Ordered by specified columns.</li>
<li>VACUUM FULL without exclusive lock (packing rows only).</li>
<li>Rebuild or relocate only the indexes of a table.</li>
</ul>
<h2>Why rebuild a table (and/or indexes)?</h2>
<blockquote>
<p>VACUUM is one of the key maintenance operations in PostgreSQL, responsible
for cleaning up dead tuples from the table pages, thereby freeing up space.
This space can either be re-used for future insertions and updates or remain
unused and lead to fragmentation. With massive data purge operations
involving deletion of a significant percentage of data from a table, the
amount of fragmentation may be significantly larger. Eventually, these
fragmented pages increase the size of the table and utilize more disk space
than required. Such fragmentation might distribute tuples across an
unnecessarily large number of pages, creating performance bottlenecks.
<a href="https://hexacluster.ai/postgresql/rebuilding-tables-online-using-pg_repack-in-postgresql/">source</a></p>
</blockquote>
<p>The problem with VACUUM FULL is that it blocks reads and writes.</p>
<p>An article worth reading is Amazon's &quot;Reducing bloat in tables and indexes with
pg_repac <a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Appendix.PostgreSQL.CommonDBATasks.pg_repack.html">source</a></p>
<h2>Requirements</h2>
<ul>
<li>To ensure efficient performance, the target table should possess either a
primary key or at least a unique index covering a non-null column.</li>
<li>Executing a complete table repack necessitates available disk space roughly
twice the size of the target table(s) and their associated indexes.</li>
<li>pg_repack will not work on postgresql catalog tables.</li>
<li>Only table owners and superusers can use pg_repack to rebuild the table.</li>
</ul>
<h2>Example</h2>
<p>This is how you'd rebuild a table using pg_repack</p>
<pre><code class="language-sh"># Perform a dry-run pass to see whether we can rebuild the table online
$ pg_repack --table foo -d my_db --dry-run

# To execute the rebuild
$ pg_repack --table foo -d my_db
INFO: repacking table &quot;public.foo&quot;

# It may throw this warning if the table doesn't have a PK or unique index
# non-nullable column:
WARNING: relation &quot;public.foo&quot; must have a primary key or not-null unique keys
</code></pre>
<p>Useful flags:</p>
<ul>
<li>-e: gives more verbose information</li>
<li>--wait-timeout: adds sensible lock timeout values. Defaults to 60s.</li>
<li>--no-kill-backend: do not kill queries blocking pg_repack.</li>
<li>--jobs: for parallelism. Add if you have spare CPUs.</li>
</ul>
<p>A useful flag is <code>-e</code>, it will give more verbose information back to the
caller.</p>
<p>There are 7 steps performed by pg_repack here:</p>
<ol>
<li><strong>Create a log table</strong>: This table will log inserts, updates, and deletes to
the old table.</li>
<li><strong>Add triggers</strong>: The trigger will capture inserts, updates, and deletes and
insert them into the log table created before.
The table looks like this (when it has data in it):<pre><code> id |    pk    |                          row
----+----------+--------------------------------------------------------
  1 |          | (100012,my_name001,42,&quot;2025-02-04 18:14:53.780287+13&quot;)
  2 |          | (100013,my_name003,42,&quot;2025-02-04 18:14:53.785157+13&quot;)
  3 | (100013) | (100013,my_name003,43,&quot;2025-02-04 18:14:53.785157+13&quot;)
  4 | (100012) |
  5 | (100013) |
</code></pre>
The above means that rows 100012 and 100013 were inserted and then deleted
(empty data in the row).</li>
<li><strong>Create new table</strong>: Create the new table.</li>
<li><strong>Add indexes in the new table</strong>: Add index on the PK for query performance.</li>
<li><strong>Apply changes from log table</strong>: For synchronisation.</li>
<li><strong>Swaps the tables</strong>: And inform the catalogue.</li>
<li><strong>Drop old table</strong>.</li>
</ol>
<h2>Important use case</h2>
<p>Sometimes you need to rebuild an old table, creating a new table, and swap it
by the new one.</p>
<p>One such case I've come across in production is that someone wanted to add an
EXCLUSION constraint to a table with over 100 million rows and a lot of access
across the codebase. EXCLUSION constraints can't be created without blocking
reads and writes. In this case, a new table needed to be created, with the new
exclusion constraint, and swapped with the old one.</p>
<h2>Installing from source</h2>
<p>I prefer to build things from source whenever I can.
If you built postgres from source, you'll have to also install pg_repack from
source as well.</p>
<pre><code class="language-sh">POSTGRES_DIR=$HOME/workspace/postgres15
REPACK_VERSION=1.5.1

cd $POSTGRES_DIR
# I use the &quot;build&quot; folder for building artifacts:
cd build/

git clone git@github.com:reorg/pg_repack.git
cd pg_repack
git checkout tags/ver_$REPACK_VERSION

# PG repack needs to know where `pg_config` is. Remove the &quot;bear&quot; part if you
# don't want a compile_commands.json file at the end.
PG_CONFIG=$POSTGRES_DIR/build/bin/pg_config bear --output compile_commands.json -- make
PG_CONFIG=$POSTGRES_DIR/build/bin/pg_config make install

# To use the binary (command line) copy it to the postgres directory so that
# all binaries are in the same place.
cp bin/pg_repack $POSTGRES_DIR/build/bin
</code></pre>
<p>Now hop on <code>psql</code> and create the extension in the database of choice:</p>
<pre><code class="language-sql">CREATE EXTENSION pg_repack;
</code></pre>
<h2>If you prefer docker</h2>
<p>Here's some instructions to run it via docker.</p>
<ul>
<li><a href="https://github.com/hartmut-co-uk/pg-repack-docker">pg-repack-docker</a></li>
</ul>
<h2>Testing</h2>
<p>Create tables and data:</p>
<p>Note: The bellow requires the <code>btree_gist</code> extension to be installed.</p>
<pre><code class="language-sql">DROP TABLE IF EXISTS table_for_repack;
CREATE TABLE table_for_repack (
    id SERIAL PRIMARY KEY,
    name VARCHAR(255) NOT NULL,
    value INTEGER NOT NULL,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    CONSTRAINT value_check CHECK (value &gt;= 0),
    CONSTRAINT name_unique UNIQUE (name)
);

CREATE INDEX idx_value ON table_for_repack (value);
CREATE INDEX idx_created_at ON table_for_repack (created_at);

-- Insert 100_000 rows
INSERT INTO table_for_repack (name, value)
SELECT
    'item_' || generate_series(1, 100000) AS name,
    (generate_series(1, 100000) % 100000) + 1 AS value;

DROP TABLE IF EXISTS table_with_fk;
CREATE TABLE table_with_fk (
    id SERIAL PRIMARY KEY,
    table_for_repack_id INTEGER NOT NULL,
    CONSTRAINT fk_table_for_repack FOREIGN KEY (table_for_repack_id)
        REFERENCES table_for_repack(id)
        ON DELETE CASCADE
        ON UPDATE CASCADE
);

-- Insert 100_000 rows
INSERT INTO table_with_fk (table_for_repack_id)
SELECT generate_series(1, 100000);
</code></pre>
<p>Now let's run pg_repack in and print the statements:</p>
<pre><code class="language-sh">POSTGRES_DIR=$HOME/workspace/postgres15
PG_REPACK_BIN=$POSTGRES_DIR/build/bin/pg_repack

TABLE=table_for_repack
DB_NAME=repack_test

$PG_REPACK_BIN --table $TABLE -d $DB_NAME --echo --elevel=DEBUG
</code></pre>
<p>This is what I get in return:</p>
<pre><code>$PG_REPACK_BIN --table table_for_repack -d repack_test --echo --elevel=DEBUG
Process 67229 launched: '/Users/marcelo.fernandes/workspace/postgres15/build/bin/pg_repack' (arm64)
DEBUG: No workers to disconnect.
LOG: (query) SET search_path TO pg_catalog, pg_temp, public
LOG: (query) SET search_path TO pg_catalog, pg_temp, public
LOG: (query) select repack.version(), repack.version_sql()
LOG: (query) SET statement_timeout = 0
LOG: (query) SET search_path = pg_catalog, pg_temp, public
LOG: (query) SET client_min_messages = warning
LOG: (query) SELECT r FROM (VALUES ($1, 'r')) AS given_t(r,kind) WHERE NOT EXISTS(  SELECT FROM repack.tables WHERE relid=to_regclass(given_t.r)) AND NOT EXISTS(  SELECT FROM pg_catalog.pg_class c WHERE c.oid=to_regclass(given_t.r) AND c.relkind = given_t.kind AND given_t.kind = 'p')
LOG:    (param:0) = table_for_repack
LOG: (query) SELECT t.*, coalesce(v.tablespace, t.tablespace_orig) as tablespace_dest FROM repack.tables t,  (VALUES ($1::text)) as v (tablespace) WHERE (relid = $2::regclass) ORDER BY t.relname, t.schemaname
LOG:    (param:0) = (null)
LOG:    (param:1) = table_for_repack
INFO: repacking table &quot;public.table_for_repack&quot;
DEBUG: ---- repack_one_table ----
DEBUG: target_name       : public.table_for_repack
DEBUG: target_oid        : 2094620
DEBUG: target_toast      : 0
DEBUG: target_tidx       : 0
DEBUG: pkid              : 2094626
DEBUG: ckid              : 0
DEBUG: create_pktype     : SELECT repack.create_index_type(2094626,2094620)
DEBUG: create_log        : SELECT repack.create_log_table(2094620)
DEBUG: create_trigger    : CREATE TRIGGER repack_trigger AFTER INSERT OR DELETE OR UPDATE ON public.table_for_repack FOR EACH ROW EXECUTE PROCEDURE repack.repack_trigger('id')
DEBUG: enable_trigger    : ALTER TABLE public.table_for_repack ENABLE ALWAYS TRIGGER repack_trigger
DEBUG: create_table      : SELECT repack.create_table($1, $2)
DEBUG: dest_tablespace   : pg_default
DEBUG: copy_data         : INSERT INTO repack.table_2094620 SELECT id,name,value,created_at FROM ONLY public.table_for_repack
DEBUG: alter_col_storage : (skipped)
DEBUG: drop_columns      : (skipped)
DEBUG: delete_log        : DELETE FROM repack.log_2094620
DEBUG: lock_table        : LOCK TABLE public.table_for_repack IN ACCESS EXCLUSIVE MODE
DEBUG: sql_peek          : SELECT * FROM repack.log_2094620 ORDER BY id LIMIT $1
DEBUG: sql_insert        : INSERT INTO repack.table_2094620 VALUES ($1.*)
DEBUG: sql_delete        : DELETE FROM repack.table_2094620 WHERE (id) = ($1.id)
DEBUG: sql_update        : UPDATE repack.table_2094620 SET (id, name, value, created_at) = ($2.id, $2.name, $2.value, $2.created_at) WHERE (id) = ($1.id)
DEBUG: sql_pop           : DELETE FROM repack.log_2094620 WHERE id IN (
DEBUG: ---- setup ----
LOG: (query) SELECT pg_try_advisory_lock($1, CAST(-2147483648 + $2::bigint AS integer))
LOG:    (param:0) = 16185446
LOG:    (param:1) = 2094620
LOG: (query) BEGIN ISOLATION LEVEL READ COMMITTED
LOG: (query) SET LOCAL lock_timeout = 100
LOG: (query) LOCK TABLE public.table_for_repack IN ACCESS EXCLUSIVE MODE
LOG: (query) RESET lock_timeout
LOG: (query) SELECT pg_get_indexdef(indexrelid) FROM pg_index WHERE indrelid = $1 AND NOT indisvalid
LOG:    (param:0) = 2094620
LOG: (query) SELECT indexrelid, repack.repack_indexdef(indexrelid, indrelid, $2, FALSE)  FROM pg_index WHERE indrelid = $1 AND indisvalid
LOG:    (param:0) = 2094620
LOG:    (param:1) = (null)
DEBUG: index[0].target_oid      : 2094626
DEBUG: index[0].create_index    : CREATE UNIQUE INDEX index_2094626 ON repack.table_2094620 USING btree (id)
DEBUG: index[1].target_oid      : 2094628
DEBUG: index[1].create_index    : CREATE UNIQUE INDEX index_2094628 ON repack.table_2094620 USING btree (name)
DEBUG: index[2].target_oid      : 2094630
DEBUG: index[2].create_index    : CREATE INDEX index_2094630 ON repack.table_2094620 USING btree (value)
DEBUG: index[3].target_oid      : 2094631
DEBUG: index[3].create_index    : CREATE INDEX index_2094631 ON repack.table_2094620 USING btree (created_at)
LOG: (query) SELECT repack.conflicted_triggers($1)
LOG:    (param:0) = 2094620
LOG: (query) SELECT repack.create_index_type(2094626,2094620)
LOG: (query) SELECT repack.create_log_table(2094620)
LOG: (query) CREATE TRIGGER repack_trigger AFTER INSERT OR DELETE OR UPDATE ON public.table_for_repack FOR EACH ROW EXECUTE PROCEDURE repack.repack_trigger('id')
LOG: (query) ALTER TABLE public.table_for_repack ENABLE ALWAYS TRIGGER repack_trigger
LOG: (query) SELECT repack.disable_autovacuum('repack.log_2094620')
LOG: (query) BEGIN ISOLATION LEVEL READ COMMITTED
LOG: (query) SELECT pg_backend_pid()
DEBUG: LOCK TABLE public.table_for_repack IN ACCESS SHARE MODE
LOG: (query) SELECT pid FROM pg_locks WHERE locktype = 'relation' AND granted = false AND relation = 2094620 AND mode = 'AccessExclusiveLock' AND pid &lt;&gt; pg_backend_pid()
DEBUG: No competing DDL to cancel.
LOG: (query) COMMIT
DEBUG: Waiting on ACCESS SHARE lock...
DEBUG: ---- copy tuples ----
LOG: (query) BEGIN ISOLATION LEVEL SERIALIZABLE
LOG: (query) SELECT set_config('work_mem', current_setting('maintenance_work_mem'), true)
LOG: (query) SELECT coalesce(array_agg(l.virtualtransaction), '{}')   FROM pg_locks AS l   LEFT JOIN pg_stat_activity AS a     ON l.pid = a.pid   LEFT JOIN pg_database AS d     ON a.datid = d.oid   WHERE l.locktype = 'virtualxid'   AND l.pid NOT IN (pg_backend_pid(), $1)   AND (l.virtualxid, l.virtualtransaction) &lt;&gt; ('1/1', '-1/0')   AND (a.application_name IS NULL OR a.application_name &lt;&gt; $2)  AND a.query !~* E'^\\s*vacuum\\s+'   AND a.query !~ E'^autovacuum: '   AND ((d.datname IS NULL OR d.datname = current_database()) OR l.database = 0)
LOG:    (param:0) = 67243
LOG:    (param:1) = pg_repack
LOG: (query) DELETE FROM repack.log_2094620
LOG: (query) SAVEPOINT repack_sp1
LOG: (query) SELECT pid FROM pg_locks WHERE locktype = 'relation' AND granted = false AND relation = 2094620 AND mode = 'AccessExclusiveLock' AND pid &lt;&gt; pg_backend_pid()
DEBUG: No competing DDL to cancel.
LOG: (query) SET LOCAL lock_timeout = 100
LOG: (query) LOCK TABLE public.table_for_repack IN ACCESS SHARE MODE
LOG: (query) RESET lock_timeout
LOG: (query) SELECT repack.create_table($1, $2)
LOG:    (param:0) = 2094620
LOG:    (param:1) = pg_default
LOG: (query) INSERT INTO repack.table_2094620 SELECT id,name,value,created_at FROM ONLY public.table_for_repack
LOG: (query) SELECT repack.disable_autovacuum('repack.table_2094620')
LOG: (query) COMMIT
LOG: (query) SELECT 'repack.table_2094620'::regclass::oid
DEBUG: ---- create indexes ----
DEBUG: Have 4 indexes and num_workers=0
DEBUG: set up index_jobs [0]
DEBUG: target_oid   : 2094626
DEBUG: create_index : CREATE UNIQUE INDEX index_2094626 ON repack.table_2094620 USING btree (id)
LOG: (query) CREATE UNIQUE INDEX index_2094626 ON repack.table_2094620 USING btree (id)
DEBUG: set up index_jobs [1]
DEBUG: target_oid   : 2094628
DEBUG: create_index : CREATE UNIQUE INDEX index_2094628 ON repack.table_2094620 USING btree (name)
LOG: (query) CREATE UNIQUE INDEX index_2094628 ON repack.table_2094620 USING btree (name)
DEBUG: set up index_jobs [2]
DEBUG: target_oid   : 2094630
DEBUG: create_index : CREATE INDEX index_2094630 ON repack.table_2094620 USING btree (value)
LOG: (query) CREATE INDEX index_2094630 ON repack.table_2094620 USING btree (value)
DEBUG: set up index_jobs [3]
DEBUG: target_oid   : 2094631
DEBUG: create_index : CREATE INDEX index_2094631 ON repack.table_2094620 USING btree (created_at)
LOG: (query) CREATE INDEX index_2094631 ON repack.table_2094620 USING btree (created_at)
LOG: (query) SELECT repack.repack_apply($1, $2, $3, $4, $5, $6)
LOG:    (param:0) = SELECT * FROM repack.log_2094620 ORDER BY id LIMIT $1
LOG:    (param:1) = INSERT INTO repack.table_2094620 VALUES ($1.*)
LOG:    (param:2) = DELETE FROM repack.table_2094620 WHERE (id) = ($1.id)
LOG:    (param:3) = UPDATE repack.table_2094620 SET (id, name, value, created_at) = ($2.id, $2.name, $2.value, $2.created_at) WHERE (id) = ($1.id)
LOG:    (param:4) = DELETE FROM repack.log_2094620 WHERE id IN (
LOG:    (param:5) = 1000
LOG: (query) SELECT pid FROM pg_locks WHERE locktype = 'virtualxid' AND pid &lt;&gt; pg_backend_pid() AND virtualtransaction = ANY($1)
LOG:    (param:0) = {}
DEBUG: ---- swap ----
LOG: (query) SAVEPOINT repack_sp1
LOG: (query) SET LOCAL lock_timeout = 100
LOG: (query) LOCK TABLE public.table_for_repack IN ACCESS EXCLUSIVE MODE
LOG: (query) RESET lock_timeout
LOG: (query) SAVEPOINT repack_sp1
LOG: (query) SET LOCAL lock_timeout = 100
LOG: (query) LOCK TABLE repack.table_2094620 IN ACCESS EXCLUSIVE MODE
LOG: (query) RESET lock_timeout
LOG: (query) SELECT repack.repack_apply($1, $2, $3, $4, $5, $6)
LOG:    (param:0) = SELECT * FROM repack.log_2094620 ORDER BY id LIMIT $1
LOG:    (param:1) = INSERT INTO repack.table_2094620 VALUES ($1.*)
LOG:    (param:2) = DELETE FROM repack.table_2094620 WHERE (id) = ($1.id)
LOG:    (param:3) = UPDATE repack.table_2094620 SET (id, name, value, created_at) = ($2.id, $2.name, $2.value, $2.created_at) WHERE (id) = ($1.id)
LOG:    (param:4) = DELETE FROM repack.log_2094620 WHERE id IN (
LOG:    (param:5) = 0
LOG: (query) SELECT repack.repack_swap($1)
LOG:    (param:0) = 2094620
LOG: (query) COMMIT
DEBUG: ---- drop ----
LOG: (query) BEGIN ISOLATION LEVEL READ COMMITTED
LOG: (query) SAVEPOINT repack_sp1
LOG: (query) SET LOCAL lock_timeout = 100
LOG: (query) LOCK TABLE public.table_for_repack IN ACCESS EXCLUSIVE MODE
LOG: (query) RESET lock_timeout
LOG: (query) SELECT repack.repack_drop($1, $2)
LOG:    (param:0) = 2094620
LOG:    (param:1) = 4
LOG: (query) COMMIT
DEBUG: ---- analyze ----
LOG: (query) BEGIN ISOLATION LEVEL READ COMMITTED
LOG: (query) ANALYZE public.table_for_repack
LOG: (query) COMMIT
LOG: (query) SELECT pg_advisory_unlock($1, CAST(-2147483648 + $2::bigint AS integer))
LOG:    (param:0) = 16185446
LOG:    (param:1) = 2094620
DEBUG: No workers to disconnect.
Process 67229 exited with status = 0 (0x00000000)
</code></pre>
<p>TODO: Check log of all queries on postgres side</p>
<h2>Going deeper, what does the pg_repack binary actually do?</h2>
<p>The best way to understand this is by reading through the code of the <code>main</code>
function that gets executed when the binary runs. The code can be accessed
<a href="https://github.com/reorg/pg_repack/blob/a4b9b33e488a2e1585b26c9425824ada88c84a2f/bin/pg_repack.c#L300-L300">here</a>.</p>
<p>Straight away, this function uses the <code>pgut</code> library a lot. This library
is present also in <code>pg_reorg</code>. The commit that added it into <code>pg_repack</code>
(574b6dc2) only states &quot;Support PGXS&quot;.</p>
<p>In summary:</p>
<ol>
<li>Initialise <code>pgut</code> with the command line arguments (via <code>pgut_getopt</code>). This
will set the program name to <code>pg_repack</code>, set the postgres locally for the
command so that it can run from the scripts folder, and set up interruption
handlers so that pgut disconnects at the end.</li>
<li>Check for errors in the command line arguments provided.</li>
<li>Check if the tablespace requested existed (a location on disk where data
files will be stored). Potentially controlled via the command line argument
<code>--tablespace=TBLSPC</code>. Performs this query:<pre><code class="language-sql">select spcname from pg_tablespace where spcname = $1;
</code></pre>
</li>
<li>Verify if should order by a particular column (<code>orderby</code>).</li>
<li>Finally call <code>repack_one_database</code> or <code>repack_all_databases</code>.</li>
</ol>
<h2>Repacking &quot;one database&quot; (repack_one_database)</h2>
<p>This is just a wrapper for repacking a table.
It will collate a few bits of info:</p>
<ul>
<li>Number of parent tables to handle.</li>
<li>Number of tables to handle.</li>
<li>Number of extensions to exclude (tables belonging to extensions).</li>
</ul>
<p>And then will try to do a few things:</p>
<ul>
<li>Set up multiple workers in case concurrent index rebuild is required. Or when
repacking many tables at a time.</li>
<li>Perform sanity checks before beginning work.<ul>
<li>Making sure pg_repack is installed in the db and versions match.</li>
<li>the user is a superuser (available from the connection is_superuser
details),</li>
<li><code>SET statement_timeout = 0;</code></li>
<li><code>SET search_path = pg_catalog, pg_temp, public;</code></li>
<li><code>SET client_min_messages = warning;</code> (gets rid of annoying &quot;create
implicit...&quot; messages)</li>
</ul>
</li>
<li>Check if the relations to be repacked exist and if the repack tables are
already there:<pre><code class="language-sql">--  creates a temporary table with two columns: r and kind.
-- $1 is a placeholder for a parameter that will be provided when the
-- query is executed. In this case, it's the table name &quot;table_for_repack&quot;.
-- AS given_t(r, kind) renames the columns of the temporary values table to r
-- and kind.
-- given_t is the alias of the temporary table.
-- 
SELECT r FROM (VALUES ($1, 'r')) AS given_t(r,kind)
WHERE NOT EXISTS (
    SELECT FROM repack.tables
    WHERE relid=to_regclass(given_t.r)
) AND NOT EXISTS (
    SELECT FROM pg_catalog.pg_class c
    WHERE c.oid=to_regclass(given_t.r)
    AND c.relkind = given_t.kind
    -- Check if the kind is 'p' (which stands for partitioned tables in
    -- PostgreSQL). So, this is filtering for partitioned tables only.
    -- Partitioned tables aren't supported.
    AND given_t.kind = 'p'
)
</code></pre>
<pre><code class="language-sql">SELECT t.*, coalesce(v.tablespace, t.tablespace_orig) as tablespace_dest
FROM repack.tables t,
(VALUES ($1::text)) as v (tablespace)
WHERE (relid = $2::regclass) ORDER BY t.relname, t.schemaname
</code></pre>
</li>
<li>It requires the relation must have a primary key or not-null unique key (for
the batch lookup).</li>
<li>It then builds the &quot;table&quot; variable of type <code>repack_table</code>. This is an
example of what it holds in our sandbox case:<pre><code>(repack_table) {
  target_name = 0x00000001398087d8 &quot;public.table_for_repack&quot;
  target_oid = 2093608
  target_toast = 0
  target_tidx = 0
  pkid = 2093614
  ckid = 0
  temp_oid = 0
  create_pktype = 0x000000013980880b &quot;SELECT repack.create_index_type(2093614,2093608)&quot;
  create_log = 0x000000013980883c &quot;SELECT repack.create_log_table(2093608)&quot;
  create_trigger = 0x0000000139808864 &quot;CREATE TRIGGER repack_trigger AFTER INSERT OR DELETE OR UPDATE ON public.table_for_repack FOR EACH ROW EXECUTE PROCEDURE repack.repack_trigger('id')&quot;
  enable_trigger = 0x00000001398088f9 &quot;ALTER TABLE public.table_for_repack ENABLE ALWAYS TRIGGER repack_trigger&quot;
  create_table = 0x0000000139808942 &quot;SELECT repack.create_table($1, $2)&quot;
  dest_tablespace = 0x0000000139808b8b &quot;pg_default&quot;
  copy_data = 0x0000000139808970 &quot;INSERT INTO repack.table_2093608 SELECT id,name,value,created_at FROM ONLY public.table_for_repack&quot;
  alter_col_storage = 0x0000000000000000
  drop_columns = 0x0000000000000000
  delete_log = 0x00000001398089d3 &quot;DELETE FROM repack.log_2093608&quot;
  lock_table = 0x0000000139808a08 &quot;LOCK TABLE public.table_for_repack IN ACCESS EXCLUSIVE MODE&quot;
  sql_peek = 0x0000000139808a44 &quot;SELECT * FROM repack.log_2093608 ORDER BY id LIMIT $1&quot;
  sql_insert = 0x0000000139808a7a &quot;INSERT INTO repack.table_2093608 VALUES ($1.*)&quot;
  sql_delete = 0x0000000139808aa9 &quot;DELETE FROM repack.table_2093608 WHERE (id) = ($1.id)&quot;
  sql_update = 0x0000000139808adf &quot;UPDATE repack.table_2093608 SET (id, name, value, created_at) = ($2.id, $2.name, $2.value, $2.created_at) WHERE (id) = ($1.id)&quot;
  sql_pop = 0x0000000139808b5e &quot;DELETE FROM repack.log_2093608 WHERE id IN (&quot;
  n_indexes = 0
  indexes = NULL
}
</code></pre>
</li>
<li>Then finally calls <code>repack_one_table</code>.</li>
</ul>
<h2>Repacking &quot;one table&quot; (repack_one_table)</h2>
<p>This function effectivelly performs the repacking.
During its set up, it will print the following under DEBUG mode:</p>
<pre><code>INFO: repacking table &quot;public.table_for_repack&quot;
DEBUG: ---- repack_one_table ----
DEBUG: target_name       : public.table_for_repack
DEBUG: target_oid        : 2093608
DEBUG: target_toast      : 0
DEBUG: target_tidx       : 0
DEBUG: pkid              : 2093614
DEBUG: ckid              : 0
DEBUG: create_pktype     : SELECT repack.create_index_type(2093614,2093608)
DEBUG: create_log        : SELECT repack.create_log_table(2093608)
DEBUG: create_trigger    : CREATE TRIGGER repack_trigger AFTER INSERT OR DELETE OR UPDATE ON public.table_for_repack FOR EACH ROW EXECUTE PROCEDURE repack.repack_trigger('id')
DEBUG: enable_trigger    : ALTER TABLE public.table_for_repack ENABLE ALWAYS TRIGGER repack_trigger
DEBUG: create_table      : SELECT repack.create_table($1, $2)
DEBUG: dest_tablespace   : pg_default
DEBUG: copy_data         : INSERT INTO repack.table_2093608 SELECT id,name,value,created_at FROM ONLY public.table_for_repack
DEBUG: alter_col_storage : (skipped)
DEBUG: drop_columns      : (skipped)
DEBUG: delete_log        : DELETE FROM repack.log_2093608
DEBUG: lock_table        : LOCK TABLE public.table_for_repack IN ACCESS EXCLUSIVE MODE
DEBUG: sql_peek          : SELECT * FROM repack.log_2093608 ORDER BY id LIMIT $1
DEBUG: sql_insert        : INSERT INTO repack.table_2093608 VALUES ($1.*)
DEBUG: sql_delete        : DELETE FROM repack.table_2093608 WHERE (id) = ($1.id)
DEBUG: sql_update        : UPDATE repack.table_2093608 SET (id, name, value, created_at) = ($2.id, $2.name, $2.value, $2.created_at) WHERE (id) = ($1.id)
DEBUG: sql_pop           : DELETE FROM repack.log_2093608 WHERE id IN (
</code></pre>
<p>Then:</p>
<ol>
<li>It will try to acquire an advisory lock to forbid multiple runs of
<code>pg_repack</code> of happening on the same table.</li>
<li>It will lock the <code>table_for_repack</code> in access exclusive mode. This helps
with doing <code>pg_get_indexdef</code> calls (which require an access share lock
anyway) and making sure they won't be blocked.<pre><code class="language-sql">SELECT pg_get_indexdef(indexrelid)
FROM pg_index WHERE indrelid = $1 AND NOT indisvalid
 -- LOG: (query) SELECT pg_get_indexdef(indexrelid) FROM pg_index
 -- WHERE indrelid = $1 AND NOT indisvalid
 -- LOG: (param:0) = 2093608
</code></pre>
This query checks if there's any invalid index present on the table, and
raises an error if <code>--error-on-invalid-index</code> is not set.</li>
<li>Now check for the valid indexes...<pre><code class="language-sql">SELECT indexrelid, repack.repack_indexdef(indexrelid, indrelid, $2, FALSE)
FROM pg_index WHERE indrelid = $1 AND indisvalid
-- LOG:    (param:0) = 2093608
</code></pre>
This query will store the <code>indexrelid</code> information, and the
<code>repack_indexdef</code> function figures out what the definition of that index has
to be for the table to be repacked.</li>
<li>It will try to clean up the &quot;repack trigger&quot; on the table if a previous
attempt at repacking has failed and needs clean up. A cool thing about
extensions is that if you perform DROP EXTENSION, it will automatically
delete these triggers and clean up everything nicely. And then you can just
CREATE EXTENSION again and you're all set to retry.</li>
<li>Create the pktype (index) <code>SELECT repack.create_index_type(2093614,2093608)</code>.</li>
<li>Create the log table: <code>SELECT repack.create_log_table(2093608)</code>.</li>
<li>Create the trigger<pre><code class="language-sql">CREATE TRIGGER repack_trigger AFTER INSERT OR DELETE OR UPDATE ON
public.table_for_repack
FOR EACH ROW EXECUTE PROCEDURE repack.repack_trigger('id')
</code></pre>
</li>
<li>Enable the trigger <code>ALTER TABLE public.table_for_repack ENABLE ALWAYS TRIGGER repack_trigger</code>.</li>
<li>Disable autovacuum: <code>SELECT repack.disable_autovacuum('repack.log_2093608')</code>
I'm not sure why this is needed. Worth checking history of this (see
bin/pg_repack.c:1389)</li>
<li>Ask for another connection (async) to lock the <code>table_for_repack</code> in
AccessShare mode to avoid DDLs trying to change that table of running
before we repack. That would affect our repacking process. DDLs are killed
if that's the case.
<code>LOCK TABLE public.table_for_repack IN ACCESS SHARE MODE</code>.</li>
<li>Now, we need to kill other ddls (except that by conn2 that just acquired
the lock above), so that they don't acquire the lock before conn2 can!</li>
<li>Finally close that transaction from early on with ACCESS EXCLUSIVE mode,
now the second connection will keep holding the AccessShare lock.</li>
</ol>
<p>Now the process of starting to backfill starts</p>
<ol>
<li>BEGIN ISOLATION LEVEL SERIALIZABLE (this is used to avoid race condition
between the create_table statement and rows subsequently being added to the
log table).</li>
<li><code>SET work_mem = maintanence_work_mem</code>
<code>SELECT set_config('work_mem', current_setting('maintenance_work_mem'), true)&quot;, 0, NULL);</code></li>
<li>Fetch all active transaction virtual ids (see SQL_XID_SNAPSHOT_90200 for query).</li>
<li>Delete any existing entries in the log table since CREATE TABLE ... AS
SELECT hasn't run yet. <code>DELETE FROM repack.log_2093608</code>. This truncates the
table.</li>
<li>Obtain access share lock on the target table for the create_table command to
go through. It's possible another DDL is in the lock queue now after conn2,
so kill that first if so!</li>
<li>Before copying data, it changes the column storage type if its storage type
has been changed from the type <code>default</code>. (Not sure when this happens).</li>
<li>Table is created as <code>SELECT repack.create_table($1, $2)</code><pre><code>LOG: (query) SELECT repack.create_table($1, $2)
LOG:    (param:0) = 2093608
LOG:    (param:1) = pg_default
</code></pre>
</li>
<li>And then data is copied:<pre><code class="language-sql">INSERT INTO repack.table_2093608 SELECT id,name,value,created_at
FROM ONLY public.table_for_repack
</code></pre>
</li>
<li>Disables autovacuum on the table, not sure why yet!
<code>SELECT repack.disable_autovacuum('repack.table_2093608')</code></li>
<li>Grab oid of the existing table
<code>SELECT 'repack.table_2093608'::regclass::oid</code></li>
<li>Creates the indexes on the destination table</li>
<li>Keeps running <code>apply_log</code> on a ethernal loop which runs<pre><code>LOG: (query) SELECT repack.repack_apply($1, $2, $3, $4, $5, $6)
LOG:    (param:0) = SELECT * FROM repack.log_2093608 ORDER BY id LIMIT $1
LOG:    (param:1) = INSERT INTO repack.table_2093608 VALUES ($1.*)
LOG:    (param:2) = DELETE FROM repack.table_2093608 WHERE (id) = ($1.id)
LOG:    (param:3) = UPDATE repack.table_2093608 SET (id, name, value, created_at) = ($2.id, $2.name, $2.value, $2.created_at) WHERE (id) = ($1.id)
LOG:    (param:4) = DELETE FROM repack.log_2093608 WHERE id IN (
LOG:    (param:5) = 1000
</code></pre>
TODO: Learn about <code>apply_log</code>.</li>
<li>Swaps the tables using conn2 since it already has the AcessShare lock. It
uses repack_swap.</li>
<li>Analyses the table <code>ANALYZE public.table_for_repack</code> (why?)</li>
<li>Calls <code>repack_drop</code></li>
</ol>
<h2>repack_apply</h2>
<p>The function has 6 arguments and returns the number of performed operations.</p>
<pre><code>* @param	sql_peek	SQL to pop tuple from log table.
* @param	sql_insert	SQL to insert into temp table.
* @param	sql_delete	SQL to delete from temp table.
* @param	sql_update	SQL to update temp table.
* @param	sql_pop	SQL to bulk-delete tuples from log table.
* @param	count		Max number of operations, or no count iff &lt;=0.
* @retval				Number of performed operations.
</code></pre>
<p>In my example:</p>
<pre><code>sql_peek: SELECT * FROM repack.log_2093608 ORDER BY id LIMIT $1
sql_insert: INSERT INTO repack.table_2093608 VALUES ($1.*)
sql_delete: DELETE FROM repack.table_2093608 WHERE (id) = ($1.id)
sql_update: UPDATE repack.table_2093608 SET (id, name, value, created_at) = (
                $2.id, $2.name, $2.value, $2.created_at
            ) WHERE (id) = ($1.id)
sql_pop: DELETE FROM repack.log_2093608 WHERE id IN (
count: 1000
</code></pre>
<p>The <code>repack_prepare</code> function parses and prepares the SQL to be executed, but
doesn't run it. It's the job of <code>execute_plan</code> to do so.</p>
<h2>Debugging</h2>
<p>I've used this script to quickly recompile <code>pg_repack</code>:</p>
<pre><code>run_lldb() {
    POSTGRES_DIR=$HOME/workspace/postgres15
    REPACK_VERSION=1.5.1

    cd $POSTGRES_DIR
    cd build/
    cd pg_repack

    PG_CONFIG=$POSTGRES_DIR/build/bin/pg_config make
    PG_CONFIG=$POSTGRES_DIR/build/bin/pg_config make install

    cp bin/pg_repack $POSTGRES_DIR/build/bin

    PG_REPACK_BIN=$POSTGRES_DIR/build/bin/pg_repack

    TABLE=table_for_repack
    DB_NAME=repack_test

    lldb -- $PG_REPACK_BIN
}
run_lldb

(lldb) b b
(lldb) run --table table_for_repack -d repack_test --echo --elevel=DEBUG
</code></pre>
<p>Additionally, some of the queries triggered from inside the extension don't
really show up on the logs. I had to manually hack the places where I wanted
to debug with <code>elog</code> calls such as:</p>
<pre><code class="language-c">    elog(LOG, &quot;[REPACK_TEST] SPI_execute: %s&quot;, sql);
</code></pre>
<p>For this to take effect, remember to recompile the C project and then:</p>
<pre><code class="language-sql">DROP EXTENSION pg_repack CASCADE; CREATE EXTENSION pg_repack;
</code></pre>
<p>Also, if I wanted to pause the program I dropped some <code>sleep()</code> calls, this was
helpful when I wanted to throw some more data in the table_to_repack while the
repack apply process was running.</p>
<pre><code class="language-sql">INSERT INTO table_for_repack (name, value) VALUES ('my_name000', 42);
INSERT INTO table_for_repack (name, value) VALUES ('my_name001', 42);
INSERT INTO table_for_repack (name, value) VALUES ('my_name003', 42);
UPDATE table_for_repack SET value = 43 WHERE name = 'my_name003';
DELETE FROM table_for_repack WHERE name = 'my_name001';
DELETE FROM table_for_repack WHERE name = 'my_name002';
DELETE FROM table_for_repack WHERE name = 'my_name003';
-- This has to be run very fast after these update queries!
SELECT * FROM repack.log_2094620;
--
-- id |    pk    |                          row
------+----------+--------------------------------------------------------
--  1 |          | (100012,my_name001,42,&quot;2025-02-04 18:14:53.780287+13&quot;)
--  2 |          | (100013,my_name003,42,&quot;2025-02-04 18:14:53.785157+13&quot;)
--  3 | (100013) | (100013,my_name003,43,&quot;2025-02-04 18:14:53.785157+13&quot;)
--  4 | (100012) |
--  5 | (100013) |
</code></pre>
<script async src="https://scripts.simpleanalyticscdn.com/latest.js"></script>
</body>
</html>
